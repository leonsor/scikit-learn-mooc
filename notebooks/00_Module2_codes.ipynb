{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Python functions used during Scikit-learn MOOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "data, target = housing.data, housing.target\n",
    "target *= 100  # rescale the target in k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "cv_results = cross_validate(regressor, data, target,\n",
    "                            cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "                            return_train_score=True, n_jobs=2)\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation used the negative mean absolute error. We transform\n",
    "the negative mean absolute error into a positive mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "scores[[\"train error\", \"test error\"]] = -cv_results[\n",
    "    [\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores.plot.hist(bins=50, edgecolor=\"black\")\n",
    "plt.xlabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Train and test errors distribution via cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Validation curve\n",
    "\n",
    "Some model hyperparameters are usually the key to go from a model that\n",
    "underfits to a model that overfits, hopefully going through a region were we\n",
    "can get a good balance between the two. We can acquire knowledge by plotting\n",
    "a curve called the validation curve. This curve can also be applied to the\n",
    "above experiment and varies the value of a hyperparameter.\n",
    "\n",
    "For the decision tree, the `max_depth` parameter is used to control the\n",
    "tradeoff between under-fitting and over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "max_depth = [1, 5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    regressor, data, target, param_name=\"max_depth\", param_range=max_depth,\n",
    "    cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depth, train_errors.mean(axis=1), label=\"Training error\")\n",
    "plt.plot(max_depth, test_errors.mean(axis=1), label=\"Testing error\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Maximum depth of decision tree\")\n",
    "plt.ylabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Validation curve for decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve\n",
    "\n",
    "Therefore, we can vary the number of samples in the training set and repeat\n",
    "the experiment. The training and testing scores can be plotted similarly to\n",
    "the validation curve, but instead of varying a hyperparameter, we vary the\n",
    "number of training samples. This curve is called the **learning curve**.\n",
    "\n",
    "It gives information regarding the benefit of adding new training samples\n",
    "to improve a model's generalization performance.\n",
    "\n",
    "Let's compute the learning curve for a decision tree and vary the\n",
    "proportion of the training set from 10% to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_sizes = np.linspace(0.1, 1.0, num=5, endpoint=True)\n",
    "train_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a `ShuffleSplit` cross-validation to assess our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "results = learning_curve(\n",
    "    regressor, data, target, train_sizes=train_sizes, cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "# Convert the scores into errors\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.errorbar(train_size, train_errors.mean(axis=1),\n",
    "             yerr=train_errors.std(axis=1), label=\"Training error\")\n",
    "plt.errorbar(train_size, test_errors.mean(axis=1),\n",
    "             yerr=test_errors.std(axis=1), label=\"Testing error\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of samples in the training set\")\n",
    "plt.ylabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Learning curve for decision tree\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
